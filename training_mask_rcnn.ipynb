{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbdElRahmanFarhan/box_semantic_segmentation/blob/main/training_mask_rcnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "539c4fcb",
      "metadata": {
        "id": "539c4fcb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "id": "axyPOQG1PhoC"
      },
      "id": "axyPOQG1PhoC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !cp /content/drive/MyDrive/OSCD.zip /content/\n",
        "# !unzip /content/OSCD.zip -d /content/OSCD/"
      ],
      "metadata": {
        "id": "CfQz8nRQFi5u"
      },
      "id": "CfQz8nRQFi5u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "zip_file_path = '/content/drive/MyDrive/OSCD.zip'\n",
        "dataset_folder = '/content/drive/MyDrive/OSCD/'\n",
        "\n",
        "if len(os.listdir(dataset_folder)) == 0:\n",
        "  with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "      zip_ref.extractall(dataset_folder)\n",
        "\n",
        "  print(f\"Unzipped to: {dataset_folder}\")"
      ],
      "metadata": {
        "id": "tVioBrKYUq-B"
      },
      "id": "tVioBrKYUq-B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_folder = os.path.join(dataset_folder, 'coco_carton/oneclass_carton/images/train2017')\n",
        "val_folder = os.path.join(dataset_folder, 'coco_carton/oneclass_carton/images/val2017')\n",
        "train_annotation = os.path.join(dataset_folder, 'coco_carton/oneclass_carton/annotations/instances_train2017.json')\n",
        "val_annotation = os.path.join(dataset_folder, 'coco_carton/oneclass_carton/annotations/instances_val2017.json')"
      ],
      "metadata": {
        "id": "I1nBV1DvVSIY"
      },
      "id": "I1nBV1DvVSIY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision.datasets import CocoDetection\n",
        "from PIL import Image\n",
        "from torchvision.tv_tensors import Mask\n",
        "from torchvision.transforms.functional import to_tensor\n",
        "from skimage.draw import polygon as sk_polygon\n",
        "from torchvision import tv_tensors\n",
        "from torchvision.transforms.v2 import functional as F\n",
        "\n",
        "class OSCDDataset(CocoDetection):\n",
        "    def __init__(self, img_folder, ann_file, transforms=None):\n",
        "        super().__init__(img_folder, ann_file, transforms=None)\n",
        "        self._transforms = transforms\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "       return super().__len__()\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, anns = super().__getitem__(idx)\n",
        "\n",
        "        if anns:\n",
        "          labels = []\n",
        "          areas = []\n",
        "          iscrowd = []\n",
        "          masks = []\n",
        "          boxes = []\n",
        "          ids = []\n",
        "          for ann in anns:\n",
        "              x, y, w, h = list(map(int, ann['bbox']))\n",
        "              boxes.append([x, y, x + w, y + h])\n",
        "              labels.append(ann['category_id'])\n",
        "              areas.append(ann['area'])\n",
        "              iscrowd.append(ann['iscrowd'])\n",
        "              mask = self.get_mask(ann['segmentation'], img.size[1], img.size[0])\n",
        "              masks.append(mask)\n",
        "              ids.append(ann['id'])\n",
        "\n",
        "          labels = torch.tensor(labels, dtype=torch.int64)\n",
        "          areas = torch.tensor(areas, dtype=torch.float16)\n",
        "          iscrowd = torch.tensor(iscrowd, dtype=torch.uint8)\n",
        "          boxes = torch.tensor(boxes, dtype=torch.int64)\n",
        "          boxes = tv_tensors.BoundingBoxes(boxes, format=\"XYXY\", canvas_size=F.get_size(img))\n",
        "          masks = torch.stack(masks, dim=0)\n",
        "          ids = torch.tensor(ids, dtype=torch.int64)\n",
        "          img_id = torch.tensor(ann['image_id'], dtype=torch.int64)\n",
        "\n",
        "          target = {\n",
        "              \"boxes\": boxes,\n",
        "              \"labels\": labels,\n",
        "              \"image_id\": img_id,\n",
        "              \"ids\": ids,\n",
        "              \"area\": areas,\n",
        "              \"iscrowd\": iscrowd,\n",
        "              \"masks\": Mask(masks),\n",
        "          }\n",
        "          img = tv_tensors.Image(img)\n",
        "          if self._transforms is not None:\n",
        "            img, target = self._transforms(img, target)\n",
        "          return img, target\n",
        "\n",
        "        else:\n",
        "          target = {\n",
        "            'boxes': torch.zeros((0, 4), dtype=torch.float32),\n",
        "            'labels': torch.zeros(0, dtype=torch.int64),\n",
        "            'masks': torch.zeros((0, img.size[1], img.size[0]), dtype=torch.bool),\n",
        "            'area': torch.zeros(0, dtype=torch.float32),\n",
        "            'iscrowd': torch.zeros(0, dtype=torch.int64),\n",
        "          }\n",
        "          img = tv_tensors.Image(img)\n",
        "          img = v2.ToDtype(torch.float, scale=True)(img)\n",
        "          return img, target\n",
        "\n",
        "\n",
        "    def get_mask(self, segmentation, height, width):\n",
        "        mask = torch.zeros((height, width), dtype=torch.bool)\n",
        "        poly_x = segmentation[0][::2]\n",
        "        poly_y = segmentation[0][1::2]\n",
        "        rr, cc = sk_polygon(poly_y, poly_x, shape=(height, width))\n",
        "        mask[rr, cc] = 1\n",
        "        return mask\n"
      ],
      "metadata": {
        "id": "b8nnnWtw4QZ_"
      },
      "id": "b8nnnWtw4QZ_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms import v2\n",
        "\n",
        "# def collate_fn(batch):\n",
        "#     images, targets = [], []\n",
        "#     for (image, target) in batch:\n",
        "#       if not target:\n",
        "#         continue\n",
        "#       else:\n",
        "#         images.append(image)\n",
        "#         targets.append(target)\n",
        "#     return images, targets\n",
        "\n",
        "collate_fn = lambda batch: tuple(zip(*batch))\n",
        "\n",
        "def get_transforms(train=False):\n",
        "  transforms = []\n",
        "  if train:\n",
        "    # transforms.append(v2.RandomZoomOut(p=0.7, side_range=(1.0, 1.2), fill=0))\n",
        "    transforms.append(v2.RandomHorizontalFlip(p=0.5))\n",
        "    # transforms.append(v2.RandomVerticalFlip(p=0.5))\n",
        "    # transforms.append(v2.RandomRotation(degrees=(-180, 180)))\n",
        "    transforms.append(v2.RandomPerspective(distortion_scale=0.2, p=0.5))\n",
        "    # transforms.append(v2.RandomCrop(size=(512, 512), pad_if_needed=True))\n",
        "    # transforms.append(v2.RandomIoUCrop(min_scale=0.5))\n",
        "    transforms.append(v2.SanitizeBoundingBoxes())\n",
        "\n",
        "  transforms.append(v2.ToDtype(torch.float32, scale=True))\n",
        "  transforms.append(v2.ToPureTensor())\n",
        "  return v2.Compose(transforms)\n",
        "\n",
        "train_dataset = OSCDDataset(train_folder, train_annotation, transforms=get_transforms(train=True))\n",
        "val_dataset = OSCDDataset(val_folder, val_annotation, transforms=get_transforms())\n",
        "\n",
        "train_dataset_small = torch.utils.data.Subset(train_dataset, list(range(2000)))\n",
        "val_dataset_small = torch.utils.data.Subset(val_dataset, list(range(100)))"
      ],
      "metadata": {
        "id": "lfqBVyFN7_eL"
      },
      "id": "lfqBVyFN7_eL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "from torchvision.models.detection.rpn import RegionProposalNetwork\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "\n",
        "\n",
        "def create_model(num_classes=2,\n",
        "                 rpn_fg_iou_thresh=0.7,\n",
        "                 rpn_bg_iou_thresh=0.3,\n",
        "                 rpn_batch_size_per_image=256,\n",
        "                 rpn_positive_fraction=0.5,\n",
        "                 rpn_nms_thresh=0.7,\n",
        "                 rpn_pre_nms_top_n_train=2000,\n",
        "                 rpn_pre_nms_top_n_test=1000,\n",
        "                 rpn_post_nms_top_n_train=2000,\n",
        "                 rpn_post_nms_top_n_test=1000,\n",
        "                 rpn_score_thresh=0\n",
        "                 ):\n",
        "\n",
        "  model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
        "  in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "  model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "  in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
        "  model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, 256, num_classes)\n",
        "  old_rpn = model.rpn\n",
        "\n",
        "  post_nms_top_n = {\"training\": rpn_pre_nms_top_n_train, \"testing\": rpn_pre_nms_top_n_test}\n",
        "  pre_nms_top_n = {\"training\": rpn_pre_nms_top_n_train, \"testing\": rpn_pre_nms_top_n_test}\n",
        "\n",
        "  # anchor_sizes = ((32,), (64,), (128,), (256,), (512,))\n",
        "  # aspect_ratios = ((0.5, 1.0, 2.0),) * len(anchor_sizes)\n",
        "  # anchor_generator = AnchorGenerator(sizes=anchor_sizes, aspect_ratios=aspect_ratios)\n",
        "  anchor_generator = old_rpn.anchor_generator\n",
        "  new_rpn = RegionProposalNetwork(\n",
        "          anchor_generator=anchor_generator,\n",
        "          head=old_rpn.head,\n",
        "          fg_iou_thresh=rpn_fg_iou_thresh,\n",
        "          bg_iou_thresh=rpn_bg_iou_thresh,\n",
        "          batch_size_per_image=rpn_batch_size_per_image,\n",
        "          positive_fraction=rpn_positive_fraction,\n",
        "          nms_thresh=rpn_nms_thresh,\n",
        "          post_nms_top_n=post_nms_top_n,\n",
        "          pre_nms_top_n=pre_nms_top_n,\n",
        "          score_thresh=rpn_score_thresh\n",
        "          )\n",
        "  model.rpn = new_rpn\n",
        "  return model"
      ],
      "metadata": {
        "id": "UT10dgbvFYSc"
      },
      "id": "UT10dgbvFYSc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "id": "SvSSH7HbPIK_"
      },
      "id": "SvSSH7HbPIK_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from torch.amp import autocast, GradScaler\n",
        "\n",
        "def run_epoch(model, dataloader, optimizer, device, scaler, is_training):\n",
        "    model.train()\n",
        "    progress_bar = tqdm(total=len(dataloader), desc=\"Train\" if is_training else \"Valid\")  # Initialize a progress bar\n",
        "    epoch_total_loss = 0.\n",
        "    epoch_losses = {\n",
        "      'loss_classifier': 0,\n",
        "      'loss_box_reg': 0.,\n",
        "      'loss_mask': 0.,\n",
        "      'loss_objectness': 0.,\n",
        "      'loss_rpn_box_reg': 0.}\n",
        "    num_batches = 0\n",
        "    for batch_id, (images, targets) in enumerate(dataloader):\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "        # if len(targets) == 0:\n",
        "        #   continue\n",
        "        images = [image.to(device) for image in images]\n",
        "        num_batches += 1\n",
        "\n",
        "        with autocast(device_type=device.type, dtype=torch.bfloat16):\n",
        "            if is_training:\n",
        "                losses = model(images, targets)\n",
        "            else:\n",
        "                with torch.no_grad():\n",
        "                    losses = model(images, targets)\n",
        "\n",
        "            total_loss = sum([loss for loss in losses.values()])\n",
        "\n",
        "        if is_training:\n",
        "            optimizer.zero_grad()\n",
        "            if scaler:\n",
        "                scaler.scale(total_loss).backward()\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                total_loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        epoch_losses = {k: v.item() + epoch_losses[k] for k, v in losses.items()}\n",
        "        epoch_total_loss += total_loss.item()\n",
        "        progress_bar_dict = dict(avg_loss=epoch_total_loss/(num_batches+1))\n",
        "        progress_bar.set_postfix(progress_bar_dict)\n",
        "        progress_bar.update()\n",
        "        if is_training:\n",
        "          assert not math.isnan(total_loss.item()) and math.isfinite(total_loss.item()), \"Loss is NaN or infinite. Stopping training.\"\n",
        "    progress_bar.close()\n",
        "    epoch_losses = {k: v/(num_batches + 1) for k, v in epoch_losses.items()}\n",
        "    return epoch_losses"
      ],
      "metadata": {
        "id": "oCIJ0KD1PLPZ"
      },
      "id": "oCIJ0KD1PLPZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "    'method': 'random'\n",
        "    }\n",
        "\n",
        "metric = {\n",
        "    'name': 'valid/loss',\n",
        "    'goal': 'minimize'\n",
        "    }\n",
        "\n",
        "sweep_config['metric'] = metric\n",
        "parameters_dict = {\n",
        "    'epochs': {\n",
        "        'values': [60]\n",
        "        },\n",
        "    'lr': {\n",
        "        'values': [5e-3]\n",
        "        },\n",
        "    'weight_decay': {\n",
        "          'values': [5e-4]\n",
        "        },\n",
        "    'bs': {\n",
        "          'values': [2]\n",
        "        },\n",
        "    'save_model_every': {\n",
        "          'values': [10]\n",
        "        },\n",
        "    'scheduler': {\n",
        "          'values': ['step']\n",
        "        },\n",
        "    'step_size': {\n",
        "          'values': [3]\n",
        "        },\n",
        "    'gamma': {\n",
        "          'values': [0.2]\n",
        "        },\n",
        "    'optimizer_type': {\n",
        "          'values': ['sgd']\n",
        "        },\n",
        "    'rpn_fg_iou_thresh': {\n",
        "          'values': [0.8] # default 0.7. increase it. be more strict to detect true positives because of overlap\n",
        "        },\n",
        "    'rpn_bg_iou_thresh': {\n",
        "      'values': [0.4] # default 0.3. increase it. increase the number of background detection\n",
        "        },\n",
        "    'rpn_batch_size_per_image': {\n",
        "      'values': [256] # default 256\n",
        "          },\n",
        "    'rpn_positive_fraction': {\n",
        "      'values': [0.5] # default 0.5\n",
        "          },\n",
        "    'rpn_nms_thresh': {\n",
        "      'values': [0.6] # default 0.7 reduce it. this will reduce overlap\n",
        "          },\n",
        "    'rpn_pre_nms_top_n_train': {\n",
        "      'values': [2000] # default 2000\n",
        "          },\n",
        "    'rpn_pre_nms_top_n_test': {\n",
        "      'values': [1000] # default 1000\n",
        "          },\n",
        "    'rpn_post_nms_top_n_train': {\n",
        "      'values': [2000] # default 2000\n",
        "          },\n",
        "    'rpn_post_nms_top_n_test': {\n",
        "      'values': [1000] # default 1000\n",
        "          },\n",
        "    'rpn_score_thresh': {\n",
        "      'values': [0] # default 0\n",
        "          }\n",
        "    }\n",
        "sweep_config['parameters'] = parameters_dict"
      ],
      "metadata": {
        "id": "OihvSf4DPNnl"
      },
      "id": "OihvSf4DPNnl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"box_segmentation\")"
      ],
      "metadata": {
        "id": "bmnZGnP3PYPn",
        "collapsed": true
      },
      "id": "bmnZGnP3PYPn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "import datetime\n",
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "\n",
        "\n",
        "num_workers = 12\n",
        "train_data = train_dataset\n",
        "val_data = val_dataset\n",
        "\n",
        "def train(config=None):\n",
        "  with wandb.init(config=config):\n",
        "    config = wandb.config\n",
        "\n",
        "    model = create_model(\n",
        "        rpn_fg_iou_thresh=config.rpn_fg_iou_thresh,\n",
        "        rpn_bg_iou_thresh=config.rpn_bg_iou_thresh,\n",
        "        rpn_batch_size_per_image=config.rpn_batch_size_per_image,\n",
        "        rpn_positive_fraction=config.rpn_positive_fraction,\n",
        "        rpn_nms_thresh=config.rpn_nms_thresh,\n",
        "        rpn_pre_nms_top_n_train=config.rpn_pre_nms_top_n_train,\n",
        "        rpn_pre_nms_top_n_test=config.rpn_pre_nms_top_n_test,\n",
        "        rpn_post_nms_top_n_train=config.rpn_post_nms_top_n_train,\n",
        "        rpn_post_nms_top_n_test=config.rpn_post_nms_top_n_test,\n",
        "        rpn_score_thresh=config.rpn_score_thresh\n",
        "    )\n",
        "    model.to(device)\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    optimizer_type = config.optimizer_type\n",
        "\n",
        "    if optimizer_type == 'sgd':\n",
        "      optimizer = torch.optim.SGD(params, lr=config.lr, momentum=0.9, weight_decay=config.weight_decay)\n",
        "    elif optimizer_type == 'adamw':\n",
        "      optimizer = torch.optim.AdamW(params, lr=config.lr, weight_decay=config.weight_decay)\n",
        "\n",
        "    if config.scheduler == 'step':\n",
        "      lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=config.step_size, gamma=config.gamma)\n",
        "    elif config.scheduler == 'linear':\n",
        "      lr_scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.1, total_iters=config.epochs)\n",
        "    elif config.scheduler == 'cyclic':\n",
        "      lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=config.lr, total_steps=config.epochs)\n",
        "\n",
        "\n",
        "    train_loader = DataLoader(train_data, batch_size=config.bs, shuffle=True, collate_fn=collate_fn, num_workers=num_workers)\n",
        "    val_loader = DataLoader(val_data, batch_size=config.bs, shuffle=False, collate_fn=collate_fn, num_workers=num_workers)\n",
        "\n",
        "    scaler = GradScaler()\n",
        "    for epoch in tqdm(range(config.epochs), desc=\"Epochs\"):\n",
        "\n",
        "        train_losses = run_epoch(model, train_loader, optimizer, device, scaler, is_training=True)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            valid_losses = run_epoch(model, val_loader, None, device, scaler, is_training=False)\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        train_losses = {f'train/{k}': v for k, v in train_losses.items()}\n",
        "        wandb.log(train_losses)\n",
        "        train_loss = sum(train_losses.values())\n",
        "        wandb.log({'train/loss': train_loss})\n",
        "\n",
        "        valid_losses = {f'valid/{k}': v for k, v in valid_losses.items()}\n",
        "        wandb.log(valid_losses)\n",
        "        valid_loss = sum(valid_losses.values())\n",
        "        wandb.log({'valid/loss': valid_loss})\n",
        "\n",
        "        wandb.log({'lr': lr_scheduler.get_last_lr()[0]})\n",
        "        model_name = f'model_{wandb.run.name}_{wandb.run.sweep_id}_{epoch+1}.pth'\n",
        "        if (epoch+1) % config.save_model_every == 0:\n",
        "          model_path = os.path.join(dataset_folder, 'model', model_name)\n",
        "          torch.save(model.state_dict(), model_path)"
      ],
      "metadata": {
        "id": "61q_8uJuPaqD"
      },
      "id": "61q_8uJuPaqD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.agent(sweep_id, train, count=1)"
      ],
      "metadata": {
        "id": "9JzMSBNMPeby"
      },
      "id": "9JzMSBNMPeby",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}